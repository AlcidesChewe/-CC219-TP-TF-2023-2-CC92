{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlcidesChewe/TF_MENSAJES_NEGATIVOS/blob/main/Modelo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeNHjCl3Pcxi",
    "outputId": "280270f9-427e-4f26-f27e-e83307e5b14f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/curz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/curz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # read the csv\n",
    "import re # regex to detect username, url, html entity\n",
    "import nltk # to use word tokenize (split the sentence into words)\n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6KGm-1YPsX1",
    "outputId": "b0497e46-0fdc-4f49-ba5a-53ec6ba8bb0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets: (24783, 7)\n"
     ]
    }
   ],
   "source": [
    "# initiate the path and read it\n",
    "dataset_path = \"./labeled_data.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()\n",
    "\n",
    "# dataset shape to know how many tweets in the datasets\n",
    "print(f\"num of tweets: {df.shape}\")\n",
    "\n",
    "# extract the text and labels\n",
    "tweet = list(df['tweet'])\n",
    "labels = list(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C_He4kXSPt80"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "# remove html entity:\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "# change the user tags\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove urls\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove unnecessary symbols\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "## this function in to clean all the dataset by utilizing all the function above\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HG5ALVmYPt_c"
   },
   "outputs": [],
   "source": [
    "clean_tweet = preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "D1frr192PuB1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweet, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SRdLBbiRPuEW"
   },
   "outputs": [],
   "source": [
    "## Tokenizing -> basically we use tokenisation for many things, its commonly used for feature extraction in preprocessing. btw idk how it works as feature extraction tho :(\n",
    "# declare the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# build the vocabulary based on train dataset\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenize the train and test dataset\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# vocabulary size (num of unique words) -> will be used in embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZMEXlV82fa6M"
   },
   "outputs": [],
   "source": [
    "# Cargar GloVe embeddings\n",
    "embedding_index = dict()\n",
    "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Crear la matriz de embedding\n",
    "embedding_matrix = np.zeros((vocab_size, 100))  # Asegúrate de que la dimensión aquí corresponda con la de GloVe\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Las palabras no encontradas en GloVe serán vectores de ceros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "i3FxCToCPuG0"
   },
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-JZinppQP1x8"
   },
   "outputs": [],
   "source": [
    "# create hot_labels (idk whty tapi ini penting, kalo ga bakal error)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_train = to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alWf48XjP13t",
    "outputId": "bcb23a45-7f6b-4231-a10c-a39929f89b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 4957\n",
      "num train tweet: 19826\n"
     ]
    }
   ],
   "source": [
    "# another look on the number of tweet in test and training data\n",
    "\n",
    "print(f\"num test tweet: {y_test.shape[0]}\")\n",
    "print(f\"num train tweet: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lG1goPKbP18M"
   },
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "# credits/source:\n",
    "\"\"\"\n",
    "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "\"\"\"\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DQ8N4epbP1-9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 21:57:52.378611: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# change dis if u want\n",
    "#output_dim = 200\n",
    "\n",
    "# LSTM model architechture (CNN + LSTM)\n",
    "#model = Sequential([\n",
    "    # embedding layer is like idk\n",
    "  #  Embedding(vocab_size, output_dim, input_length=max_length),\n",
    "    # lstm for xxx\n",
    "  #  LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    # dropout to prevent overfitting\n",
    " #   Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    " #   Dense(128, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    " #   Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    " #   Dense(3, activation=\"softmax\"),\n",
    "#])\n",
    "\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])\n",
    "\n",
    "# LSTM model architecture (CNN + LSTM)\n",
    "model = Sequential([\n",
    "    # Cambia la capa de embedding para usar los embeddings preentrenados de GloVe\n",
    "    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "    # lstm for xxx\n",
    "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    "    Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1, precision, recall])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uIDVmKZP2Bl",
    "outputId": "84a1c28c-741e-43dd-9130-416751a2de9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 26, 100)           1867100   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1918047 (7.32 MB)\n",
      "Trainable params: 50947 (199.01 KB)\n",
      "Non-trainable params: 1867100 (7.12 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# checking the model parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmReJUw5PuJW",
    "outputId": "e2deb869-3d73-4d7d-ede6-aee04b484ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "310/310 [==============================] - 11s 27ms/step - loss: 0.5058 - accuracy: 0.8224 - f1: 0.8059 - precision: 0.8451 - recall: 0.7792 - val_loss: 0.3553 - val_accuracy: 0.8697 - val_f1: 0.8695 - val_precision: 0.8844 - val_recall: 0.8554\n",
      "Epoch 2/10\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.3781 - accuracy: 0.8638 - f1: 0.8622 - precision: 0.8836 - recall: 0.8421 - val_loss: 0.3314 - val_accuracy: 0.8733 - val_f1: 0.8752 - val_precision: 0.8910 - val_recall: 0.8602\n",
      "Epoch 3/10\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.3474 - accuracy: 0.8730 - f1: 0.8703 - precision: 0.8893 - recall: 0.8524 - val_loss: 0.3138 - val_accuracy: 0.8800 - val_f1: 0.8782 - val_precision: 0.9021 - val_recall: 0.8560\n",
      "Epoch 4/10\n",
      "310/310 [==============================] - 7s 23ms/step - loss: 0.3302 - accuracy: 0.8773 - f1: 0.8753 - precision: 0.8923 - recall: 0.8592 - val_loss: 0.3080 - val_accuracy: 0.8798 - val_f1: 0.8806 - val_precision: 0.9007 - val_recall: 0.8616\n",
      "Epoch 5/10\n",
      "310/310 [==============================] - 7s 23ms/step - loss: 0.3192 - accuracy: 0.8807 - f1: 0.8796 - precision: 0.8975 - recall: 0.8626 - val_loss: 0.3045 - val_accuracy: 0.8830 - val_f1: 0.8838 - val_precision: 0.9057 - val_recall: 0.8632\n",
      "Epoch 6/10\n",
      "310/310 [==============================] - 7s 23ms/step - loss: 0.3093 - accuracy: 0.8826 - f1: 0.8820 - precision: 0.8990 - recall: 0.8659 - val_loss: 0.3005 - val_accuracy: 0.8866 - val_f1: 0.8866 - val_precision: 0.9053 - val_recall: 0.8689\n",
      "Epoch 7/10\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.3017 - accuracy: 0.8857 - f1: 0.8847 - precision: 0.9002 - recall: 0.8700 - val_loss: 0.2974 - val_accuracy: 0.8890 - val_f1: 0.8891 - val_precision: 0.9019 - val_recall: 0.8769\n",
      "Epoch 8/10\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.2946 - accuracy: 0.8882 - f1: 0.8873 - precision: 0.9012 - recall: 0.8741 - val_loss: 0.2914 - val_accuracy: 0.8903 - val_f1: 0.8909 - val_precision: 0.9051 - val_recall: 0.8773\n",
      "Epoch 9/10\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.2854 - accuracy: 0.8935 - f1: 0.8918 - precision: 0.9051 - recall: 0.8791 - val_loss: 0.2935 - val_accuracy: 0.8899 - val_f1: 0.8915 - val_precision: 0.9044 - val_recall: 0.8791\n",
      "Epoch 10/10\n",
      "310/310 [==============================] - 7s 23ms/step - loss: 0.2841 - accuracy: 0.8945 - f1: 0.8930 - precision: 0.9061 - recall: 0.8805 - val_loss: 0.2927 - val_accuracy: 0.8905 - val_f1: 0.8907 - val_precision: 0.9025 - val_recall: 0.8793\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jgpJGGyPQE7O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curz/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./pretrained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4957, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "X_new = tokenizer.texts_to_sequences([\"Hello\"])\n",
    "prediction= model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1294494"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0][0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNolpCHZJAxCW1Geyz0Y0U/",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
